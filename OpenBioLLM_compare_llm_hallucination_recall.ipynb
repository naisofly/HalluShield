{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOfw0lqfGFCGi7if9ZY0YaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naisofly/HalluShield/blob/main/OpenBioLLM_compare_llm_hallucination_recall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets pandas scikit-learn\n",
        "!pip install -U datasets fsspec"
      ],
      "metadata": {
        "id": "oFC9SZQXA9zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-dAafy7AR7x"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# ----------------------------\n",
        "# GPU Setup in Google Colab\n",
        "# ----------------------------\n",
        "\n",
        "# Verify GPU availability\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# ----------------------------\n",
        "# Hugging Face Authentication\n",
        "# ----------------------------\n",
        "\n",
        "# Hugging Face authentication - replace with your token\n",
        "HF_TOKEN = \"ADD_HF_TOKEN_HERE\"  # Get from https://huggingface.co/settings/tokens\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN  # Set as environment variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1: Load & Prep the MedHallu dataset"
      ],
      "metadata": {
        "id": "EUBwuQj3BUjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset contains medical questions, hallucinated answers, and ground truth answers.\n",
        "def prepare_test_data():\n",
        "    \"\"\"Create balanced test set from hard hallucinations\"\"\"\n",
        "    ds = load_dataset(\"UTAustin-AIHealth/MedHallu\", \"pqa_labeled\")\n",
        "    df = ds['train'].to_pandas()\n",
        "\n",
        "    hard_samples = df[df['Difficulty Level'] == 'hard']\n",
        "    num_samples = len(hard_samples)\n",
        "\n",
        "    return pd.concat([\n",
        "        hard_samples.sample(num_samples, random_state=42)\n",
        "            .assign(answer=lambda x: x['Ground Truth'], label='non-hallucination'),\n",
        "        hard_samples.sample(num_samples, random_state=84)\n",
        "            .assign(answer=lambda x: x['Hallucinated Answer'], label='hallucination')\n",
        "    ]).sample(frac=1, random_state=126).reset_index(drop=True)\n",
        "\n",
        "test_df = prepare_test_data()\n",
        "print(\"Test dataset labels:\\n\", test_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "Q-7mjj5_A0f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create new test dataset from Hard Hallucinations"
      ],
      "metadata": {
        "id": "NVuwkQy5Bdxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Focus on challenging cases where hallucinations are harder to detect\n",
        "# hard_hallucinations = df[df['Difficulty Level'] == 'hard']\n",
        "# print(f\"Number of hard hallucination entries: {len(hard_hallucinations)}\")\n",
        "\n",
        "# # Create balanced test set\n",
        "# num_samples = len(hard_hallucinations)\n",
        "# test_df = pd.concat([\n",
        "#     hard_hallucinations.sample(n=num_samples, random_state=42)\n",
        "#     .assign(answer=lambda x: x['Ground Truth'], label='non-hallucination'),\n",
        "#     hard_hallucinations.sample(n=num_samples, random_state=84)\n",
        "#     .assign(answer=lambda x: x['Hallucinated Answer'], label='hallucination')\n",
        "# ]).sample(frac=1, random_state=126).reset_index(drop=True)\n",
        "\n",
        "# print(\"\\nLabel counts in new dataset:\")\n",
        "# print(test_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "N62iW9ysBcxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3: Initialize the LLMs to be evaluated"
      ],
      "metadata": {
        "id": "Adht7LyGBmxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_medical_model():\n",
        "    \"\"\"Initialize OpenBioLLM with correct chat template\"\"\"\n",
        "    return pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"aaditya/OpenBioLLM-Llama3-8B\",\n",
        "        model_kwargs={\n",
        "            \"torch_dtype\": torch.bfloat16,\n",
        "            \"offload_folder\": \"./offload\",  # <--- Add this line\n",
        "        },\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "medical_model = init_medical_model()"
      ],
      "metadata": {
        "id": "cjGkchUnBp-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4: Define system and user prompts for hallucination detection"
      ],
      "metadata": {
        "id": "bH3rjXTSMpOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------\n",
        "# Prompt Engineering\n",
        "# ----------------------------\n",
        "SYSTEM_PROMPT = \"\"\"You are OpenBioLLM, a medical expert developed by Saama AI Labs.\n",
        "Evaluate answers for factual accuracy using medical terminology and guidelines.\n",
        "Respond ONLY with 'Yes' or 'No'.\"\"\"\n",
        "\n",
        "def format_prompt(row):\n",
        "    system_message = (\n",
        "        \"You are OpenBioLLM, a medical expert developed by Saama AI Labs. \"\n",
        "        \"Evaluate answers for factual accuracy using medical terminology and guidelines. \"\n",
        "        \"Respond ONLY with 'Yes' or 'No'.\"\n",
        "    )\n",
        "    user_message = (\n",
        "        f\"Medical Context: {row['Ground Truth']}\\n\"\n",
        "        f\"Question: {row['Question']}\\n\"\n",
        "        f\"Answer: {row['answer']}\\n\\n\"\n",
        "        \"Does this answer contain factual inaccuracies? Respond ONLY with 'Yes' or 'No'.\"\n",
        "    )\n",
        "    prompt = (\n",
        "        \"<|system|>\\n\" + system_message + \"\\n<|end|>\\n\"\n",
        "        \"<|user|>\\n\" + user_message + \"\\n<|end|>\\n\"\n",
        "        \"<|assistant|>\\n\"\n",
        "    )\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "d1y8X2rHEmU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------\n",
        "# Evaluation Logic\n",
        "# ----------------------------\n",
        "def parse_response(response):\n",
        "    \"\"\"Robust parser for model outputs\"\"\"\n",
        "    response = response.lower().strip()\n",
        "    if not response:\n",
        "        return \"invalid\"\n",
        "    return response.split()[0][:3]  # Capture first 3 chars for 'yes/no' detection\n",
        "\n",
        "def batch_evaluate(df, model, batch_size=4):  # Adjust batch_size as needed\n",
        "    results = []\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        try:\n",
        "            batch = df.iloc[i:i+batch_size]\n",
        "            prompts = [format_prompt(row) for _, row in batch.iterrows()]\n",
        "            outputs = model(\n",
        "                prompts,\n",
        "                max_new_tokens=15,\n",
        "                do_sample=False,\n",
        "                pad_token_id=model.tokenizer.eos_token_id\n",
        "            )\n",
        "            for idx, (_, row) in enumerate(batch.iterrows()):\n",
        "                raw_text = outputs[idx][0]['generated_text'].split(\"<|assistant|>\")[-1]\n",
        "                prediction = \"Yes\" if \"yes\" in parse_response(raw_text) else \"No\"\n",
        "                # Print the first few results for inspection\n",
        "                if len(results) < 5:\n",
        "                    print(f\"\\n--- Sample {len(results)+1} ---\")\n",
        "                    print(\"Prompt:\\n\", prompts[idx])\n",
        "                    print(\"Raw model output:\\n\", raw_text)\n",
        "                    print(\"Predicted label:\", prediction)\n",
        "                    print(\"True label:\", row['label'])\n",
        "                results.append({\n",
        "                    \"question\": row[\"Question\"],\n",
        "                    \"label\": row[\"label\"],\n",
        "                    \"prediction\": prediction,\n",
        "                    \"correct\": (prediction == \"Yes\") == (row[\"label\"] == \"hallucination\")\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Batch {i//batch_size} failed: {str(e)}\")\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "bbNFY1W4Enqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ----------------------------\n",
        "# # Define Prompt Templates and Batch Processing Function\n",
        "# # ----------------------------\n",
        "\n",
        "# system_prompt = \"\"\"You are a medical hallucination detector.\n",
        "# Check if answers contain factual inaccuracies. Respond EXCLUSIVELY with 'Yes' or 'No'.\"\"\"\n",
        "\n",
        "# def generate_prompt(row):\n",
        "#     messages = [\n",
        "#         {\"role\": \"system\", \"content\": system_prompt},\n",
        "#         {\"role\": \"user\", \"content\": f\"\"\"\n",
        "#             Medical Context: {row['Ground Truth']}\n",
        "#             Question: {row['Question']}\n",
        "#             Answer to Evaluate: {row['answer']}\n",
        "\n",
        "#             Does the Answer contain any factual inaccuracies? Respond ONLY with 'Yes' or 'No'.\"\"\"}\n",
        "#     ]\n",
        "#     return messages\n",
        "\n",
        "# def parse_response(response):\n",
        "#     \"\"\"Strict parser with enhanced validation\"\"\"\n",
        "#     response = response.lower().strip()\n",
        "\n",
        "#     # Handle empty responses\n",
        "#     if not response:\n",
        "#         print(\"Warning: Empty response detected\")\n",
        "#         return \"invalid_response\"\n",
        "\n",
        "#     # Extract first meaningful word\n",
        "#     first_word = response.split()[0] if response.split() else \"\"\n",
        "\n",
        "#     # Strict validation\n",
        "#     if first_word == \"yes\":\n",
        "#         return \"yes\"\n",
        "#     elif first_word == \"no\":\n",
        "#         return \"no\"\n",
        "\n",
        "#     # Debug unexpected responses\n",
        "#     print(f\"Unexpected response: {response[:50]}\")\n",
        "#     return f\"invalid_{response[:20].replace(' ', '_')}\"\n",
        "\n",
        "# def evaluate_model(test_df, model, batch_size=8):  # Reduced batch size for T4 GPU\n",
        "#     results = []\n",
        "#     for i in range(0, len(test_df), batch_size):\n",
        "#         batch = test_df.iloc[i:i + batch_size]\n",
        "#         prompts = [generate_prompt(row) for _, row in batch.iterrows()]\n",
        "\n",
        "#         try:\n",
        "#             responses = model(\n",
        "#                 prompts,\n",
        "#                 do_sample=False,\n",
        "#                 temperature=0.0,\n",
        "#                 return_full_text=False,\n",
        "#                 pad_token_id=model.tokenizer.eos_token_id\n",
        "#             )\n",
        "\n",
        "#             for idx, (_, row) in enumerate(batch.iterrows()):\n",
        "#                 raw_response = responses[idx][0]['generated_text'].strip()\n",
        "#                 parsed = parse_response(raw_response)\n",
        "\n",
        "#                 # Debug output for analysis\n",
        "#                 debug_info = {\n",
        "#                     \"Expected\": row['label'],\n",
        "#                     \"Raw Response\": raw_response,\n",
        "#                     \"Parsed\": parsed\n",
        "#                 }\n",
        "#                 print(f\"Debug: {debug_info}\") if parsed not in ['yes', 'no'] else None\n",
        "\n",
        "#                 model_response = 'Yes' if parsed == 'yes' else 'No'\n",
        "#                 is_correct = (model_response == 'Yes') == (row['label'] == 'hallucination')\n",
        "\n",
        "#                 results.append({\n",
        "#                     \"Question\": row[\"Question\"],\n",
        "#                     \"Answer\": row[\"answer\"],\n",
        "#                     \"Label\": row[\"label\"],\n",
        "#                     \"Model Response\": model_response,\n",
        "#                     \"Raw Response\": raw_response,\n",
        "#                     \"Correct\": is_correct\n",
        "#                 })\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing batch {i//batch_size}: {str(e)}\")\n",
        "\n",
        "#     return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "90yiCJNhC3qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Evaluate Model on Hard Hallucinations"
      ],
      "metadata": {
        "id": "knMtFXeyM7wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Evaluate Model Using Batches\n",
        "# ----------------------------\n",
        "\n",
        "results = batch_evaluate(test_df, medical_model)"
      ],
      "metadata": {
        "id": "gYCIIJyYM-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Calculate Recall Scores"
      ],
      "metadata": {
        "id": "GzTyiHYEarFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calculate performance metrics\n",
        "cm = confusion_matrix(\n",
        "    results['label'].map({'hallucination':1, 'non-hallucination':0}),\n",
        "    results['prediction'].map({'Yes':1, 'No':0})\n",
        ")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nRecall: {cm[1,1]/(cm[1,1]+cm[1,0]):.2f}\")\n",
        "print(f\"Precision: {cm[1,1]/(cm[1,1]+cm[0,1]):.2f}\")\n",
        "\n",
        "results.to_csv(\"medical_model_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "Led4HLkj7lQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Performance Analysis and Results Saving\n",
        "# ----------------------------\n",
        "\n",
        "print(\"\\n6. Calculating metrics...\")\n",
        "true_labels = results['Label'].map({'hallucination': 1, 'non-hallucination': 0})\n",
        "predicted_labels = results_df['Model Response'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=['Non-Hallucination', 'Hallucination'])\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "disp.ax_.set_title(\"Confusion Matrix\\n(1=Hallucination, 0=Non-Hallucination)\")\n",
        "\n",
        "print(\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"True Positives (TP): {cm[1,1]}\")  # Correctly identified hallucinations\n",
        "print(f\"False Positives (FP): {cm[0,1]}\") # Non-hallucinations flagged as hallucinations\n",
        "print(f\"False Negatives (FN): {cm[1,0]}\") # Missed hallucinations\n",
        "print(f\"True Negatives (TN): {cm[0,0]}\")  # Correctly identified non-hallucinations\n",
        "\n",
        "precision = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0\n",
        "recall = cm[1,1] / (cm[1,1] + cm[1,0]) if (cm[1,1] + cm[1,0]) > 0 else 0\n",
        "\n",
        "print(f\"\\nPrecision: {precision:.2f} (How many flagged hallucinations were correct)\")\n",
        "print(f\"Recall: {recall:.2f} (How many actual hallucinations were detected)\")\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv(\"hallucination_evaluation_results.csv\", index=False)\n",
        "print(\"\\nResults saved to 'hallucination_evaluation_results.csv'\")"
      ],
      "metadata": {
        "id": "bQscZokONjy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}