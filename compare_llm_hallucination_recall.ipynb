{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNnmf8qhCyzyL3iOXauKEqK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naisofly/HalluShield/blob/main/compare_llm_hallucination_recall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets pandas torch"
      ],
      "metadata": {
        "id": "oFC9SZQXA9zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-dAafy7AR7x"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------\n",
        "# GPU Setup in Google Colab\n",
        "# ----------------------------\n",
        "\n",
        "# Verify GPU availability\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)  # Should show >=\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# ----------------------------\n",
        "# Hugging Face Authentication\n",
        "# ----------------------------\n",
        "\n",
        "# Hugging Face authentication - replace with your token\n",
        "HF_TOKEN = \"ADD_YOUR_HUGGINGFACE_TOKEN_HERE\"  # Get from https://huggingface.co/settings/tokens\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN  # Set as environment variable\n",
        "\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1: Load the MedHallu dataset"
      ],
      "metadata": {
        "id": "EUBwuQj3BUjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset contains medical questions, hallucinated answers, and ground truth answers.\n",
        "ds = load_dataset(\"UTAustin-AIHealth/MedHallu\", \"pqa_labeled\")\n",
        "df = ds['train'].to_pandas()"
      ],
      "metadata": {
        "id": "Q-7mjj5_A0f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Filter for hard hallucinations"
      ],
      "metadata": {
        "id": "NVuwkQy5Bdxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus on challenging cases where hallucinations are harder to detect\n",
        "hard_hallucinations = df[df['Difficulty Level'] == 'hard']\n",
        "print(f\"Number of hard hallucination entries: {len(hard_hallucinations)}\")"
      ],
      "metadata": {
        "id": "N62iW9ysBcxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3: Initialize the LLMs to be evaluated"
      ],
      "metadata": {
        "id": "Adht7LyGBmxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_model = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/gemma-2-2b-it\",\n",
        "    token=HF_TOKEN,\n",
        "    device=\"cuda\",  # Use GPU for inference\n",
        "    torch_dtype=torch.float16  # Use mixed precision for faster inference\n",
        ")"
      ],
      "metadata": {
        "id": "cjGkchUnBp-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4: Define system and user prompts for hallucination detection"
      ],
      "metadata": {
        "id": "bH3rjXTSMpOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Define Prompt Templates and Batch Processing Function\n",
        "# ----------------------------\n",
        "\n",
        "system_prompt = \"You are a hallucination checker for medical questions and answers. Check for hallucinations in answers. Answer strictly with 'Yes' or 'No'.\"\n",
        "\n",
        "def generate_user_prompts(batch):\n",
        "    \"\"\"\n",
        "    Generate user prompts for a batch of questions and hallucinated answers.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        f\"\"\"{system_prompt}\n",
        "        **Task**: Check if the provided Answer contains hallucinations by comparing it to the Ground Truth.\n",
        "\n",
        "        **Question**: {row['Question']}\n",
        "\n",
        "        **Answer to Evaluate**:\n",
        "        {row['Hallucinated Answer']}\n",
        "\n",
        "        **Ground Truth Reference**:\n",
        "        {row['Ground Truth']}\n",
        "\n",
        "        **Instruction**: Compare the Answer to the Ground Truth. If the Answer contains factual inaccuracies, contradictions,\n",
        "        or unsupported claims relative to the Ground Truth, respond with \"Yes\". If it aligns factually, respond with \"No\".\n",
        "        \"\"\"\n",
        "        for _, row in batch.iterrows()\n",
        "    ]\n",
        "\n",
        "def process_batch(batch):\n",
        "    \"\"\"\n",
        "    Process a batch of prompts through the Gemma model.\n",
        "    \"\"\"\n",
        "    prompts = generate_user_prompts(batch)\n",
        "\n",
        "    # Generate responses for all prompts in the batch\n",
        "    responses = gemma_model(\n",
        "        prompts,\n",
        "        max_new_tokens=2,  # Strictly limit output to \"Yes\"/\"No\"\n",
        "        do_sample=False,   # Disable sampling (greedy decoding)\n",
        "        batch_size=8,      # Adjust based on GPU memory (T4: 8-16)\n",
        "        top_k=1            # Optional: Explicitly enforce greedy behavior\n",
        "    )\n",
        "\n",
        "    # Process and validate responses\n",
        "    processed_responses = []\n",
        "    for idx, response in enumerate(responses):\n",
        "        # Extract generated text\n",
        "        response_text = response[0]['generated_text'].strip()\n",
        "\n",
        "        # Clean and standardize response\n",
        "        model_response = \"Yes\" if \"yes\" in response_text.lower() else \"No\"\n",
        "\n",
        "        # # Print verification information\n",
        "        # print(f\"\\nBatch Index: {idx}\")\n",
        "        # print(f\"Prompt Preview: {prompts[idx][:100]}...\")  # Show first 100 chars\n",
        "        # print(f\"Raw Response: {response}\")\n",
        "        # print(f\"Processed Response: {model_response}\")\n",
        "\n",
        "        processed_responses.append(model_response)\n",
        "\n",
        "    return processed_responses"
      ],
      "metadata": {
        "id": "90yiCJNhC3qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Evaluate Model on Hard Hallucinations"
      ],
      "metadata": {
        "id": "knMtFXeyM7wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Evaluate Model Using Batches\n",
        "# ----------------------------\n",
        "\n",
        "batch_size = 32  # Define batch size (adjust based on GPU memory)\n",
        "results = []\n",
        "\n",
        "for start_idx in range(0, len(hard_hallucinations), batch_size):\n",
        "    # Select a batch of data\n",
        "    batch = hard_hallucinations.iloc[start_idx:start_idx + batch_size]\n",
        "\n",
        "    # Process the batch through the model\n",
        "    responses = process_batch(batch)\n",
        "\n",
        "    # Validate responses and store results\n",
        "    for idx, response in enumerate(responses):\n",
        "        row = batch.iloc[idx]\n",
        "        is_correct = response == \"Yes\"  # Expected response is always 'Yes' since we're always giving hallucinated answers to the model\n",
        "\n",
        "        results.append({\n",
        "            \"Question\": row[\"Question\"],\n",
        "            \"Hallucinated Answer\": row[\"Hallucinated Answer\"],\n",
        "            \"Model Response\": response,\n",
        "            \"Correct Flagging\": is_correct,\n",
        "            \"Ground Truth Annotation\": row[\"Ground Truth\"]  # For reference only\n",
        "        })"
      ],
      "metadata": {
        "id": "gYCIIJyYM-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Calculate Recall Scores"
      ],
      "metadata": {
        "id": "GzTyiHYEarFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------\n",
        "# Performance Analysis and Results Saving\n",
        "# ----------------------------\n",
        "\n",
        "true_positives = sum(1 for r in results if r[\"Correct Flagging\"])\n",
        "false_negatives = sum(1 for r in results if not r[\"Correct Flagging\"])\n",
        "recall_score = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "\n",
        "print(f\"\\nFinal Recall Score: {recall_score:.2%}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\nSaving results to CSV...\")\n",
        "results_df.to_csv(\"gemma_hallucination_results.csv\", index=False)\n",
        "print(\"Results saved successfully!\")\n",
        "\n",
        "print(\"\\nPreview of Results:\")\n",
        "print(results_df.head())"
      ],
      "metadata": {
        "id": "bQscZokONjy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}