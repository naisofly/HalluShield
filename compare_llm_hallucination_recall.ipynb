{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyMTUe0ilXloi77wL6Q/U0XP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naisofly/HalluShield/blob/main/compare_llm_hallucination_recall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets pandas scikit-learn"
      ],
      "metadata": {
        "id": "oFC9SZQXA9zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-dAafy7AR7x"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# ----------------------------\n",
        "# GPU Setup in Google Colab\n",
        "# ----------------------------\n",
        "\n",
        "# Verify GPU availability\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# ----------------------------\n",
        "# Hugging Face Authentication\n",
        "# ----------------------------\n",
        "\n",
        "# Hugging Face authentication - replace with your token\n",
        "HF_TOKEN = \"ADD_YOUR_HUGGINGFACE_TOKEN_HERE\"  # Get from https://huggingface.co/settings/tokens\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN  # Set as environment variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1: Load the MedHallu dataset"
      ],
      "metadata": {
        "id": "EUBwuQj3BUjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset contains medical questions, hallucinated answers, and ground truth answers.\n",
        "ds = load_dataset(\"UTAustin-AIHealth/MedHallu\", \"pqa_labeled\")\n",
        "df = ds['train'].to_pandas()"
      ],
      "metadata": {
        "id": "Q-7mjj5_A0f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create new test dataset from Hard Hallucinations"
      ],
      "metadata": {
        "id": "NVuwkQy5Bdxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus on challenging cases where hallucinations are harder to detect\n",
        "hard_hallucinations = df[df['Difficulty Level'] == 'hard']\n",
        "print(f\"Number of hard hallucination entries: {len(hard_hallucinations)}\")\n",
        "\n",
        "# Create balanced test set\n",
        "num_samples = len(hard_hallucinations)\n",
        "test_df = pd.concat([\n",
        "    hard_hallucinations.sample(n=num_samples, random_state=42)\n",
        "    .assign(answer=lambda x: x['Ground Truth'], label='non-hallucination'),\n",
        "    hard_hallucinations.sample(n=num_samples, random_state=84)\n",
        "    .assign(answer=lambda x: x['Hallucinated Answer'], label='hallucination')\n",
        "]).sample(frac=1, random_state=126).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nLabel counts in new dataset:\")\n",
        "print(test_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "N62iW9ysBcxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3: Initialize the LLMs to be evaluated"
      ],
      "metadata": {
        "id": "Adht7LyGBmxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_model = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/gemma-2-2b-it\",\n",
        "    token=HF_TOKEN,\n",
        "    device=\"cuda\",  # Use GPU for inference\n",
        "    torch_dtype=torch.float16,\n",
        "    max_new_tokens=10,  # Increased to capture full responses\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    pad_token_id=2  # Gemma-specific requirement\n",
        ")"
      ],
      "metadata": {
        "id": "cjGkchUnBp-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4: Define system and user prompts for hallucination detection"
      ],
      "metadata": {
        "id": "bH3rjXTSMpOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Define Prompt Templates and Batch Processing Function\n",
        "# ----------------------------\n",
        "\n",
        "system_prompt = \"\"\"<bos>You are a medical hallucination detector.\n",
        "Check if answers contain factual inaccuracies. Respond ONLY with 'Yes' or 'No'.\"\"\"\n",
        "\n",
        "def generate_prompt(row):\n",
        "    return f\"\"\"<start_of_turn>user{system_prompt}\n",
        "            Context: {row['Ground Truth']}\n",
        "            Question: {row['Question']}\n",
        "            Answer: {row['answer']}<end_of_turn>\n",
        "            <start_of_turn>model\"\"\"\n",
        "\n",
        "def parse_response(response):\n",
        "    \"\"\"Handle edge cases and partial responses\"\"\"\n",
        "    response = response.lower().strip()\n",
        "\n",
        "    # Handle empty responses\n",
        "    if not response:\n",
        "        return \"unexpected_empty_response\"\n",
        "\n",
        "    # Capture any yes/no indication\n",
        "    if any(x in response for x in [\"yes\", \"no\"]):\n",
        "        return \"yes\" if \"yes\" in response else \"no\"\n",
        "\n",
        "    return f\"unexpected_{response[:20]}\"  # Truncate long unexpected responses\n",
        "\n",
        "def evaluate_model(test_df, model, batch_size=32):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test dataset and debug response structure.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for i in range(0, len(test_df), batch_size):\n",
        "        batch = test_df.iloc[i:i + batch_size]\n",
        "        prompts = [generate_prompt(row) for _, row in batch.iterrows()]\n",
        "\n",
        "        # Generate responses\n",
        "        responses = model(\n",
        "            prompts,\n",
        "            do_sample=False,\n",
        "            temperature=0.0,\n",
        "            return_full_text=False  # Get only generated text\n",
        "        )\n",
        "\n",
        "        # # Debug: Print the entire response object\n",
        "        # print(\"\\nDebugging responses:\")\n",
        "        # print(responses)\n",
        "\n",
        "        for idx, (_, row) in enumerate(batch.iterrows()):\n",
        "            # Extract raw response and validate structure\n",
        "            if 'generated_text' in responses[idx][0]:\n",
        "                raw_response = responses[idx][0]['generated_text'].strip()\n",
        "            else:\n",
        "                print(f\"\\nError: 'generated_text' key not found in response for prompt:\\n{prompts[idx]}\")\n",
        "                raw_response = \"INVALID RESPONSE\"\n",
        "\n",
        "            # Parse and validate response\n",
        "            parsed_response = parse_response(raw_response)\n",
        "\n",
        "            # # Debug: Print parsed response for verification\n",
        "            # print(f\"Raw Response: {raw_response}\")\n",
        "            # print(f\"Parsed Response: {parsed_response}\")\n",
        "\n",
        "            model_response = 'Yes' if parsed_response == 'yes' else 'No'\n",
        "            is_correct = (model_response == 'Yes') == (row['label'] == 'hallucination')\n",
        "\n",
        "            results.append({\n",
        "                \"Question\": row[\"Question\"],\n",
        "                \"Answer\": row[\"answer\"],\n",
        "                \"Label\": row[\"label\"],\n",
        "                \"Model Response\": model_response,\n",
        "                \"Raw Response\": raw_response,\n",
        "                \"Correct\": is_correct\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "90yiCJNhC3qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Evaluate Model on Hard Hallucinations"
      ],
      "metadata": {
        "id": "knMtFXeyM7wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Evaluate Model Using Batches\n",
        "# ----------------------------\n",
        "\n",
        "results_df = evaluate_model(test_df, gemma_model)"
      ],
      "metadata": {
        "id": "gYCIIJyYM-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Calculate Recall Scores"
      ],
      "metadata": {
        "id": "GzTyiHYEarFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Performance Analysis and Results Saving\n",
        "# ----------------------------\n",
        "\n",
        "print(\"\\n6. Calculating metrics...\")\n",
        "true_labels = results_df['Label'].map({'hallucination': 1, 'non-hallucination': 0})\n",
        "predicted_labels = results_df['Model Response'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=['Non-Hallucination', 'Hallucination'])\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "disp.ax_.set_title(\"Confusion Matrix\\n(1=Hallucination, 0=Non-Hallucination)\")\n",
        "\n",
        "print(\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"True Positives (TP): {cm[1,1]}\")  # Correctly identified hallucinations\n",
        "print(f\"False Positives (FP): {cm[0,1]}\") # Non-hallucinations flagged as hallucinations\n",
        "print(f\"False Negatives (FN): {cm[1,0]}\") # Missed hallucinations\n",
        "print(f\"True Negatives (TN): {cm[0,0]}\")  # Correctly identified non-hallucinations\n",
        "\n",
        "precision = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0\n",
        "recall = cm[1,1] / (cm[1,1] + cm[1,0]) if (cm[1,1] + cm[1,0]) > 0 else 0\n",
        "\n",
        "print(f\"\\nPrecision: {precision:.2f} (How many flagged hallucinations were correct)\")\n",
        "print(f\"Recall: {recall:.2f} (How many actual hallucinations were detected)\")\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv(\"hallucination_evaluation_results.csv\", index=False)\n",
        "print(\"\\nResults saved to 'hallucination_evaluation_results.csv'\")"
      ],
      "metadata": {
        "id": "bQscZokONjy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}