{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvEGg2/adEkwCJyzgphOlt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9196a6ce196d482f84ddb85a861a3890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_478ed8e5dbbd456f9b9beef7b8e11072",
              "IPY_MODEL_d38886650c7a4235aba6bc3c249682f5",
              "IPY_MODEL_b919bfad549f43818cecced24bb7ba7d"
            ],
            "layout": "IPY_MODEL_30c13bca277f4be293b0a8389cda0d7b"
          }
        },
        "478ed8e5dbbd456f9b9beef7b8e11072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52711cfed078433b9d1d412a2dab6a0d",
            "placeholder": "​",
            "style": "IPY_MODEL_f955d73eccc743ac9092b18bed57b6d7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d38886650c7a4235aba6bc3c249682f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847f715b1a334f26a6ffc6bd219da7fa",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_314f330028ce4d5794200c190dcb4366",
            "value": 2
          }
        },
        "b919bfad549f43818cecced24bb7ba7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e19163e66a7042e1906203a5d8837c3b",
            "placeholder": "​",
            "style": "IPY_MODEL_8eae06fc51b146a9bdd3e248b0c7140e",
            "value": " 2/2 [00:34&lt;00:00, 14.37s/it]"
          }
        },
        "30c13bca277f4be293b0a8389cda0d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52711cfed078433b9d1d412a2dab6a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f955d73eccc743ac9092b18bed57b6d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "847f715b1a334f26a6ffc6bd219da7fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314f330028ce4d5794200c190dcb4366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e19163e66a7042e1906203a5d8837c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eae06fc51b146a9bdd3e248b0c7140e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets pandas torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFC9SZQXA9zn",
        "outputId": "c478a84b-51b2-4cbb-d869-15ffc27732f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-dAafy7AR7x",
        "outputId": "183286ac-2d08-4bba-b7b3-4e4563035787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.3.0+cu121\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------\n",
        "# GPU Setup in Google Colab\n",
        "# ----------------------------\n",
        "\n",
        "# Verify GPU availability\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)  # Should show 2.3.0\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# ----------------------------\n",
        "# Hugging Face Authentication\n",
        "# ----------------------------\n",
        "\n",
        "# Hugging Face authentication - replace with your token\n",
        "HF_TOKEN = \"ADD_YOUR_HUGGINGFACE_TOKEN_HERE\"  # Get from https://huggingface.co/settings/tokens\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN  # Set as environment variable\n",
        "\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1: Load the MedHallu dataset"
      ],
      "metadata": {
        "id": "EUBwuQj3BUjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset contains medical questions, hallucinated answers, and ground truth answers.\n",
        "ds = load_dataset(\"UTAustin-AIHealth/MedHallu\", \"pqa_labeled\")\n",
        "df = ds['train'].to_pandas()"
      ],
      "metadata": {
        "id": "Q-7mjj5_A0f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c56daf3-7461-4994-d0e5-a15f56d36477"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Filter for hard hallucinations"
      ],
      "metadata": {
        "id": "NVuwkQy5Bdxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus on challenging cases where hallucinations are harder to detect\n",
        "hard_hallucinations = df[df['Difficulty Level'] == 'hard']\n",
        "print(f\"Number of hard hallucination entries: {len(hard_hallucinations)}\")"
      ],
      "metadata": {
        "id": "N62iW9ysBcxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4000da75-44d3-4d14-bf6d-72bb9c5e97c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of hard hallucination entries: 408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3: Initialize the LLMs to be evaluated"
      ],
      "metadata": {
        "id": "Adht7LyGBmxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_model = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/gemma-2-2b-it\",\n",
        "    token=HF_TOKEN,\n",
        "    device=\"cuda\",  # Use GPU for inference\n",
        "    torch_dtype=torch.float16  # Use mixed precision for faster inference\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9196a6ce196d482f84ddb85a861a3890",
            "478ed8e5dbbd456f9b9beef7b8e11072",
            "d38886650c7a4235aba6bc3c249682f5",
            "b919bfad549f43818cecced24bb7ba7d",
            "30c13bca277f4be293b0a8389cda0d7b",
            "52711cfed078433b9d1d412a2dab6a0d",
            "f955d73eccc743ac9092b18bed57b6d7",
            "847f715b1a334f26a6ffc6bd219da7fa",
            "314f330028ce4d5794200c190dcb4366",
            "e19163e66a7042e1906203a5d8837c3b",
            "8eae06fc51b146a9bdd3e248b0c7140e"
          ]
        },
        "id": "cjGkchUnBp-z",
        "outputId": "9064cb16-76a3-4527-cb2f-e8c0716e5ecd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9196a6ce196d482f84ddb85a861a3890"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4: Define system and user prompts for hallucination detection"
      ],
      "metadata": {
        "id": "bH3rjXTSMpOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Define Prompt Templates and Batch Processing Function\n",
        "# ----------------------------\n",
        "\n",
        "system_prompt = \"You are a hallucination checker for medical questions and answers. Check for hallucinations in answers. Answer strictly with 'Yes' or 'No'.\"\n",
        "\n",
        "def generate_user_prompts(batch):\n",
        "    \"\"\"\n",
        "    Generate user prompts for a batch of questions and hallucinated answers.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        f\"{system_prompt}\\nQuestion: {row['Question']}\\nAnswer: {row['Hallucinated Answer']}\"\n",
        "        for _, row in batch.iterrows()\n",
        "    ]\n",
        "\n",
        "def process_batch(batch):\n",
        "    \"\"\"\n",
        "    Process a batch of prompts through the Gemma model.\n",
        "    \"\"\"\n",
        "    prompts = generate_user_prompts(batch)\n",
        "\n",
        "    # Generate responses for all prompts in the batch\n",
        "    responses = gemma_model(\n",
        "        prompts,\n",
        "        max_new_tokens=2,  # Strictly limit output to \"Yes\"/\"No\"\n",
        "        do_sample=False,   # Disable sampling (greedy decoding)\n",
        "      top_k=1            # Optional: Explicitly enforce greedy behavior\n",
        "    )\n",
        "\n",
        "    # Process and validate responses\n",
        "    processed_responses = []\n",
        "    for idx, response in enumerate(responses):\n",
        "        # Extract generated text\n",
        "        response_text = response[0]['generated_text'].strip()\n",
        "\n",
        "        # Clean and standardize response\n",
        "        model_response = \"Yes\" if \"yes\" in response_text.lower() else \"No\"\n",
        "\n",
        "        # # Print verification information\n",
        "        # print(f\"\\nBatch Index: {idx}\")\n",
        "        # print(f\"Prompt Preview: {prompts[idx][:100]}...\")  # Show first 100 chars\n",
        "        # print(f\"Raw Response: {response}\")\n",
        "        # print(f\"Processed Response: {model_response}\")\n",
        "\n",
        "        processed_responses.append(model_response)\n",
        "\n",
        "    return processed_responses"
      ],
      "metadata": {
        "id": "90yiCJNhC3qr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Evaluate Model on Hard Hallucinations"
      ],
      "metadata": {
        "id": "knMtFXeyM7wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Evaluate Model Using Batches\n",
        "# ----------------------------\n",
        "\n",
        "batch_size = 32  # Define batch size (adjust based on GPU memory)\n",
        "results = []\n",
        "\n",
        "for start_idx in range(0, len(hard_hallucinations), batch_size):\n",
        "    # Select a batch of data\n",
        "    batch = hard_hallucinations.iloc[start_idx:start_idx + batch_size]\n",
        "\n",
        "    # Process the batch through the model\n",
        "    responses = process_batch(batch)\n",
        "\n",
        "    # Validate responses and store results\n",
        "    for idx, response in enumerate(responses):\n",
        "        row = batch.iloc[idx]\n",
        "        is_correct = response == \"Yes\"  # Expected response is always 'Yes' since we're always giving hallucinated answers to the model\n",
        "\n",
        "        results.append({\n",
        "            \"Question\": row[\"Question\"],\n",
        "            \"Hallucinated Answer\": row[\"Hallucinated Answer\"],\n",
        "            \"Model Response\": response,\n",
        "            \"Correct Flagging\": is_correct,\n",
        "            \"Ground Truth Annotation\": row[\"Ground Truth\"]  # For reference only\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYCIIJyYM-eI",
        "outputId": "11ce21e7-7cda-496a-c877-d291d024a9af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Calculate Recall Scores"
      ],
      "metadata": {
        "id": "GzTyiHYEarFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------\n",
        "# Performance Analysis and Results Saving\n",
        "# ----------------------------\n",
        "\n",
        "true_positives = sum(1 for r in results if r[\"Correct Flagging\"])\n",
        "false_negatives = sum(1 for r in results if not r[\"Correct Flagging\"])\n",
        "recall_score = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "\n",
        "print(f\"\\nFinal Recall Score: {recall_score:.2%}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\nSaving results to CSV...\")\n",
        "results_df.to_csv(\"gemma_hallucination_results.csv\", index=False)\n",
        "print(\"Results saved successfully!\")\n",
        "\n",
        "print(\"\\nPreview of Results:\")\n",
        "print(results_df.head())"
      ],
      "metadata": {
        "id": "bQscZokONjy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720a74ea-f8b7-49ed-bf23-b63a40ffa365"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Recall Score: 100.00%\n",
            "\n",
            "Saving results to CSV...\n",
            "Results saved successfully!\n",
            "\n",
            "Preview of Results:\n",
            "                                            Question  \\\n",
            "0  Landolt C and snellen e acuity: differences in...   \n",
            "1  Syncope during bathing in infants, a pediatric...   \n",
            "2  Can tailored interventions increase mammograph...   \n",
            "3  Is adjustment for reporting heterogeneity nece...   \n",
            "4  Do mutations causing low HDL-C promote increas...   \n",
            "\n",
            "                                 Hallucinated Answer Model Response  \\\n",
            "0  Patients with strabismus amblyopia showed a si...            Yes   \n",
            "1  Syncope during bathing in infants is a manifes...            Yes   \n",
            "2  Tailored text messages were found to be as eff...            Yes   \n",
            "3  Adjustment for reporting heterogeneity is esse...            Yes   \n",
            "4  Mutations causing low HDL-C levels do promote ...            Yes   \n",
            "\n",
            "   Correct Flagging                            Ground Truth Annotation  \n",
            "0              True  Using the charts described, there was only a s...  \n",
            "1              True  \"Aquagenic maladies\" could be a pediatric form...  \n",
            "2              True  The effects of the intervention were most pron...  \n",
            "3              True  Sleep disorders are common in the general adul...  \n",
            "4              True  Genetic variants identified in the present stu...  \n"
          ]
        }
      ]
    }
  ]
}